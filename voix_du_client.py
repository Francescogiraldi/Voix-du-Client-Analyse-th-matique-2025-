#!/usr/bin/env python3
"""
Voix du Client â€“ Analyse thÃ©matique (2025)
=========================================
Ce script rÃ©alise une .

Principales Ã©tapes
------------------
1. **Chargement** des verbatims depuis un `feedback.csv` (ou fichier uploadÃ© en UI).
2. **PrÃ©â€‘traitement** : nettoyage, normalisation, lemmatisation (spaCy `fr_core_news_sm`).
3. **Vectorisation** TFâ€‘IDF (scikitâ€‘learn) avec nâ€‘grammes (1â€“2).
4. **Clustering** Kâ€‘means (par dÃ©faut *k = 5* pour extraire les 5 irritants majeurs).
5. **SynthÃ¨se** : termes les plus reprÃ©sentatifs par cluster + volume de verbatims.
6. **Dashboard Streamlit** pour une exploration interactive.

Structure du projet
-------------------
project/
â”œâ”€â”€ voix_du_client.py          â† *ce fichier*
â”œâ”€â”€ feedback.csv               â† source verbatims (UTFâ€‘8, colonnes `id,text`)
â””â”€â”€ models/                    â† modÃ¨les/artefacts gÃ©nÃ©rÃ©s (vectoriseur, kâ€‘means)

Installation rapide
-------------------
```bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip
pip install pandas scikit-learn spacy streamlit wordcloud python-dotenv
python -m spacy download fr_core_news_sm
```

ExÃ©cution
---------
```bash
streamlit run voix_du_client.py
```
"""

from __future__ import annotations

import re
import string
from pathlib import Path
from typing import List, Tuple

import pandas as pd
import spacy
import streamlit as st
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud

# -----------------------------------------------------------------------------
# Configuration
# -----------------------------------------------------------------------------

NLP_MODEL = "fr_core_news_sm"  # modÃ¨le spaCy franÃ§ais (lÃ©ger)
STOPWORDS_EXTRA = {"nous", "vous", "ils", "elles"}  # stopwords spÃ©cifiques
DEFAULT_K = 5  # nombre de clusters par dÃ©faut
MAX_FEATURES = 5000  # vocabulaire TFâ€‘IDF
SEED = 42  # reproductibilitÃ©

# -----------------------------------------------------------------------------
# PrÃ©â€‘traitement texte
# -----------------------------------------------------------------------------

@st.cache_resource(show_spinner=False)
def load_spacy() -> spacy.language.Language:
    """Charge (et met en cache) le modÃ¨le spaCy franÃ§ais."""
    return spacy.load(NLP_MODEL)


nlp = load_spacy()


def nettoyer_texte(texte: str) -> str:
    """Nettoie et normalise un verbatim."""
    texte = texte.lower()
    texte = re.sub(r"https?://\S+", " ", texte)  # liens
    texte = re.sub(r"\d+", " ", texte)  # chiffres
    texte = texte.translate(str.maketrans("", "", string.punctuation))
    doc = nlp(texte)
    tokens = [t.lemma_ for t in doc if not (t.is_stop or t.is_punct or t.lemma_ in STOPWORDS_EXTRA)]
    return " ".join(tokens)


# -----------------------------------------------------------------------------
# Pipeline analyse
# -----------------------------------------------------------------------------

def vectoriser_textes(textes: List[str]) -> Tuple[TfidfVectorizer, "csr_matrix"]:
    """Transforme les textes en matrice TFâ€‘IDF."""
    vectorizer = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=(1, 2))
    X = vectorizer.fit_transform(textes)
    return vectorizer, X


def clusteriser(X, k: int = DEFAULT_K) -> KMeans:
    """Applique Kâ€‘means et retourne le modÃ¨le."""
    km = KMeans(n_clusters=k, init="k-means++", n_init=10, random_state=SEED)
    km.fit(X)
    return km


def termes_topics(vectorizer: TfidfVectorizer, km: KMeans, top_n: int = 10) -> List[List[str]]:
    """Retourne les termes les plus importants par cluster."""
    termes = []
    vocab = vectorizer.get_feature_names_out()
    for centre in km.cluster_centers_:
        indices = centre.argsort()[::-1][:top_n]
        termes.append([vocab[i] for i in indices])
    return termes


# -----------------------------------------------------------------------------
# Dashboard Streamlit
# -----------------------------------------------------------------------------

st.set_page_config(page_title="Voix du Client", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Analyse Voix du Client â€“ Clustering d'irritants")
st.caption("TÃ©lÃ©chargez vos verbatims clients (CSV) pour dÃ©tecter les irritants majeurs.")

fichier_upload = st.file_uploader("ğŸ“‚ Charger un fichier CSV", type=["csv"])

if fichier_upload:
    df = pd.read_csv(fichier_upload)
else:
    chemin_defaut = Path("feedback.csv")
    if chemin_defaut.exists():
        df = pd.read_csv(chemin_defaut)
        st.info("Exemple `feedback.csv` chargÃ© (dÃ©mo).")
    else:
        st.warning("Aucun fichier trouvÃ©. Veuillez uploader un CSV avec les colonnes `id,text`.")
        st.stop()

if "text" not in df.columns:
    st.error("La colonne `text` est obligatoire dans le CSV.")
    st.stop()

# Nettoyage + vectorisation
with st.spinner("Nettoyage et vectorisation des verbatimsâ€¦"):
    df["clean"] = df["text"].fillna("").apply(nettoyer_texte)
    vectorizer, X = vectoriser_textes(df["clean"].tolist())

# Choix du nombre de clusters
k = st.slider("Nombre de clusters (irritants)", 2, 10, DEFAULT_K)

if st.button("ğŸš€ Lancer l'analyse"):

    with st.spinner("Clustering en coursâ€¦"):
        km = clusteriser(X, k)
        labels = km.labels_
        df["cluster"] = labels
        score = silhouette_score(X, labels)
        termes = termes_topics(vectorizer, km)

    st.success(f"Clustering terminÃ©â€¯! Score silhouetteÂ =Â {score:.3f}")

    # Tableau rÃ©sumÃ©
    st.subheader("ğŸ“Š RÃ©sumÃ© des clusters")
    resume = (
        df.groupby("cluster")
        .size()
        .rename("nb_verbatims")
        .to_frame()
        .assign(termes=lambda d: d.index.map(lambda i: ", ".join(termes[i][:6])))
    )
    st.dataframe(resume)

    # Wordclouds par cluster
    st.subheader("â˜ï¸ Nuages de mots par cluster")
    for i in range(k):
        texts_cluster = " ".join(df.loc[df["cluster"] == i, "clean"])
        wc = WordCloud(width=800, height=400, background_color="white").generate(texts_cluster)
        st.markdown(f"##### Cluster {i} â€“ Principaux termes")
        st.image(wc.to_array(), use_column_width=True)

# -----------------------------------------------------------------------------
# Mode CLI (pour exÃ©cutions rapides)
# -----------------------------------------------------------------------------
if __name__ == "__main__" and not st.runtime.exists():
    import argparse

    parser = argparse.ArgumentParser(description="Analyse Voix du Client (CLI)")
    parser.add_argument("--csv", type=str, default="feedback.csv", help="Chemin vers le CSV de verbatims")
    parser.add_argument("--k", type=int, default=DEFAULT_K, help="Nombre de clusters")
    args = parser.parse_args()

    df_cli = pd.read_csv(args.csv)
    df_cli["clean"] = df_cli["text"].fillna("").apply(nettoyer_texte)
    _, X_cli = vectoriser_textes(df_cli["clean"].tolist())
    km_cli = clusteriser(X_cli, args.k)
    termes_cli = termes_topics(_, km_cli)

    print("\n=== RÃ©sumÃ© des clusters ===")
    for i, mots in enumerate(termes_cli):
        n = (km_cli.labels_ == i).sum()
        print(f"Cluster {i} | {n} verbatims | mots clÃ©s : {', '.join(mots[:8])}")
